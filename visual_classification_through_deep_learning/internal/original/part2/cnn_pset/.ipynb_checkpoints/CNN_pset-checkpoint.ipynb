{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Neuron Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this section, you wil be implement a simple Convolution Neuron Network(CNN) via tensorflow. You will be playing with different different architecture of the CNN and see how it affect of the performance of the CNN. In order to finish this section, you dont need to read ducomentation from tensorflow if you are not familiar with it, we will set up all the background for you and all you need to do is to see different how architectural choice affect the performance of the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the libaries and setting up the model parameters. In this Pset, you will be working with the MNIST data sets, which is a set of images of handwriting digits, 0-9, you will be using CNN to classify them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, you should have all the required libararies installed on your VM by the TA. If you see errors from the box below, please contact TA for support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MINST data\n",
    "import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "import tensorflow as tf\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the \"dropout\" is a technique to regulate the weights during the training to prevent the model overfitting. \n",
    "\n",
    "Tradiationlly, when solving an optimization problem, we use the Lagrangian multipliers and add them to the cost function. However, in the CNN, in practice, there are millions weights, therefore, in order to regulate the weights, at each layer, people randomly select a few number of neurons and \"drop\" them from the training to prevent overfiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "# decsion the Batch size for the SGD algorithm\n",
    "batch_size = 128\n",
    "#how often do we show the output\n",
    "display_step = 20\n",
    "# Network Parameters\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define covolution, ReLU layer and max pooling layer\n",
    "def conv2d(img, w, b):\n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(img, w, strides=[1, 1, 1, 1], \n",
    "                                                  padding='VALID'),b))\n",
    "\n",
    "def max_pool(img, k):\n",
    "    return tf.nn.max_pool(img, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='VALID')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the box below, we defined a CNN with 1 layer or convolution and 1 layer of max pool architecture for you. Please pay attention to how the variable is passed from \"Apply Dropout\" to the \"Fully connected layer\". Here you just need to run this code and later, we will ask to added one more convulition/ReLU layer and max pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv_net(_X, _weights, _biases, _dropout):\n",
    "    # Reshape input picture\n",
    "    _X = tf.reshape(_X, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(_X, _weights['wc1'], _biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = max_pool(conv1, k=3)\n",
    "    # Apply Dropout\n",
    "    conv1 = tf.nn.dropout(conv1, _dropout)\n",
    "\n",
    "    # Convolution Layer\n",
    "    #conv2 = conv2d(conv1, _weights['wc2'], _biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    #conv2 = max_pool(conv2, k=2)\n",
    "    # Apply Dropout\n",
    "    #conv2 = tf.nn.dropout(conv2, _dropout)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit dense layer input\n",
    "    dense1 = tf.reshape(conv1, [-1, _weights['wd1'].get_shape().as_list()[0]]) \n",
    "    # Relu activation\n",
    "    dense1 = tf.nn.relu(tf.add(tf.matmul(dense1, _weights['wd1']), _biases['bd1']))\n",
    "    # Apply Dropout\n",
    "    dense1 = tf.nn.dropout(dense1, _dropout) # Apply Dropout\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(dense1, _weights['out']), _biases['out'])\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])), \n",
    "    # fully connected, 8*8*32 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([8*8*32, 1024])), \n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes])) \n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started:\n",
      "Iter 2560, Minibatch Loss= 903.003601, Training Accuracy= 0.29688\n",
      "Iter 5120, Minibatch Loss= 535.301453, Training Accuracy= 0.51562\n",
      "Iter 7680, Minibatch Loss= 280.864624, Training Accuracy= 0.71875\n",
      "Iter 10240, Minibatch Loss= 180.920654, Training Accuracy= 0.78125\n",
      "Iter 12800, Minibatch Loss= 181.892258, Training Accuracy= 0.81250\n",
      "Iter 15360, Minibatch Loss= 181.843369, Training Accuracy= 0.80469\n",
      "Iter 17920, Minibatch Loss= 155.517273, Training Accuracy= 0.85156\n",
      "Iter 20480, Minibatch Loss= 160.072113, Training Accuracy= 0.82031\n",
      "Iter 23040, Minibatch Loss= 75.151688, Training Accuracy= 0.88281\n",
      "Iter 25600, Minibatch Loss= 119.656845, Training Accuracy= 0.89062\n",
      "Iter 28160, Minibatch Loss= 143.437347, Training Accuracy= 0.86719\n",
      "Iter 30720, Minibatch Loss= 73.735512, Training Accuracy= 0.92969\n",
      "Iter 33280, Minibatch Loss= 123.675858, Training Accuracy= 0.89844\n",
      "Iter 35840, Minibatch Loss= 73.889236, Training Accuracy= 0.90625\n",
      "Iter 38400, Minibatch Loss= 107.471237, Training Accuracy= 0.91406\n",
      "Iter 40960, Minibatch Loss= 122.509911, Training Accuracy= 0.89062\n",
      "Iter 43520, Minibatch Loss= 18.260204, Training Accuracy= 0.96875\n",
      "Iter 46080, Minibatch Loss= 95.121368, Training Accuracy= 0.92188\n",
      "Iter 48640, Minibatch Loss= 97.021782, Training Accuracy= 0.89844\n",
      "Iter 51200, Minibatch Loss= 46.658524, Training Accuracy= 0.96094\n",
      "Iter 53760, Minibatch Loss= 98.797279, Training Accuracy= 0.89062\n",
      "Iter 56320, Minibatch Loss= 33.583027, Training Accuracy= 0.92188\n",
      "Iter 58880, Minibatch Loss= 67.569557, Training Accuracy= 0.92969\n",
      "Iter 61440, Minibatch Loss= 51.289505, Training Accuracy= 0.92969\n",
      "Iter 64000, Minibatch Loss= 37.310196, Training Accuracy= 0.94531\n",
      "Iter 66560, Minibatch Loss= 80.150879, Training Accuracy= 0.92188\n",
      "Iter 69120, Minibatch Loss= 54.675697, Training Accuracy= 0.94531\n",
      "Iter 71680, Minibatch Loss= 39.438644, Training Accuracy= 0.96094\n",
      "Iter 74240, Minibatch Loss= 89.718750, Training Accuracy= 0.93750\n",
      "Iter 76800, Minibatch Loss= 63.597347, Training Accuracy= 0.94531\n",
      "Iter 79360, Minibatch Loss= 47.550240, Training Accuracy= 0.93750\n",
      "Iter 81920, Minibatch Loss= 23.317612, Training Accuracy= 0.93750\n",
      "Iter 84480, Minibatch Loss= 38.669796, Training Accuracy= 0.93750\n",
      "Iter 87040, Minibatch Loss= 22.218126, Training Accuracy= 0.96875\n",
      "Iter 89600, Minibatch Loss= 14.726208, Training Accuracy= 0.96094\n",
      "Iter 92160, Minibatch Loss= 8.290150, Training Accuracy= 0.97656\n",
      "Iter 94720, Minibatch Loss= 17.301025, Training Accuracy= 0.97656\n",
      "Iter 97280, Minibatch Loss= 23.142139, Training Accuracy= 0.94531\n",
      "Iter 99840, Minibatch Loss= 15.239841, Training Accuracy= 0.97656\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.976562\n"
     ]
    }
   ],
   "source": [
    "# Train the model and get the accuracy score\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        if step == 1:\n",
    "            print \"Training started:\"\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Fit training using batch data\n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, keep_prob: dropout})\n",
    "        \n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "            print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(acc)\n",
    "        step += 1\n",
    "    print \"Optimization Finished!\"\n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print \"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images[:256], \n",
    "                                                             y: mnist.test.labels[:256], \n",
    "                                                             keep_prob: 1.})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your coding starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add one more convolution/ReLU layer and max pooling layer in the box below. Then run the flowing code to train your model again and compare the accucracy with the CNN with only 1 convolution/ReLU layer\n",
    "\n",
    "Hint: you need to re-calculate the stride size based on the equation from the course slide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define covolution, ReLU layer and max pooling layer\n",
    "def conv2d(img, w, b):\n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(img, w, strides=[1, 1, 1, 1], \n",
    "                                                  padding='SAME'),b))\n",
    "\n",
    "def max_pool(img, k):\n",
    "    return tf.nn.max_pool(img, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(_X, _weights, _biases, _dropout):\n",
    "    # Reshape input picture\n",
    "    _X = tf.reshape(_X, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer 1\n",
    "    conv1 = conv2d(_X, _weights['wc1'], _biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = max_pool(conv1, k=)## You need to re-cacluate the stride here, which is value of k\n",
    "    # Apply Dropout\n",
    "    conv1 = tf.nn.dropout(conv1, _dropout)\n",
    "\n",
    "    ## your code here:\n",
    "    \n",
    "    # Convolution Layer 2\n",
    "    \n",
    "    # Max Pooling (down-sampling)\n",
    "    \n",
    "    # Apply Dropout\n",
    "    \n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv1 output to fit dense layer input\n",
    "    ## remember to change your input\n",
    "    \n",
    "    dense1 = tf.reshape(conv1, [-1, _weights['wd1'].get_shape().as_list()[0]]) \n",
    "    # Relu activation\n",
    "    dense1 = tf.nn.relu(tf.add(tf.matmul(dense1, _weights['wd1']), _biases['bd1']))\n",
    "    # Apply Dropout\n",
    "    dense1 = tf.nn.dropout(dense1, _dropout) # Apply Dropout\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(dense1, _weights['out']), _biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you add one more layer, then you will need to change the number of weights.\n",
    "\n",
    "Hint: you will need to add weights and bias for the new layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),  \n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])), \n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes])) \n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You don't need to edit the following code, just run them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model and get the accuracy score\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Fit training using batch data\n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, keep_prob: dropout})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "            print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(acc)\n",
    "        step += 1\n",
    "    print \"Optimization Finished!\"\n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print \"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images[:256], \n",
    "                                                             y: mnist.test.labels[:256], \n",
    "                                                             keep_prob: 1.})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you are all set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

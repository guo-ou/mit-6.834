{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from render import *\n",
    "\n",
    "def test_ok():\n",
    "    \"\"\" If execution gets to this point, print out a happy message \"\"\"\n",
    "    try:\n",
    "        from IPython.display import display_html\n",
    "        display_html(\"\"\"<div class=\"alert alert-success\">\n",
    "        <strong>Tests passed!!</strong>\n",
    "        </div>\"\"\", raw=True)\n",
    "    except:\n",
    "        print \"Tests passed!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Neural Nets\n",
    "\n",
    "In this problem set, you'll implement some of the basic functionality of neural nets as well as structure some simple nets of your own. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing Lines in Feature Space\n",
    "\n",
    "Neural networks, at their core, can be thought of as functions that draw regions in feature space. By separating the space of features into sections by drawing decision boundaries, they can be used to classify inputs. \n",
    "\n",
    "During the lecture, we focused on how deep convolutional neural networks can classify images, but in this problem set, we will be focusing on very basic, shallow neural nets. Let's take a look at an example. \n",
    "\n",
    "Given a simple two-dimentional situation, and using the step function as the activation function for the neurons, what neural net would draw the following line?\n",
    "\n",
    "y >= 5x + 3\n",
    "\n",
    "Recall that the step function is as follows: \n",
    "\n",
    "<img src=\"stepFunction.png\" width=\"300\" height=auto>\n",
    "\n",
    "We can rewrite this equation slightly to make it easier to see. \n",
    "\n",
    "y - 5x >= 3\n",
    "\n",
    "Now if we had step function with a threshold (T) value of 3 and we weighted our x input by -5, we'd get our equation!\n",
    "\n",
    "<img src=\"example1.png\" width=\"400\" height=auto>\n",
    "\n",
    "Note that we actually only need a single neuron to draw one decision boundary. In general, a neural net can draw one line for every neuron that is directly connected to the inputs. (We call these collective neurons the input layer) So, the neural network below could draw 4 decision boundaries. \n",
    "\n",
    "<img src=\"nnEx.png\" width=\"400\" height=auto>\n",
    "\n",
    "\n",
    "We can combine the regions formed by these decision boundaries using logical operations. \n",
    "\n",
    "Let's take a look at AND.\n",
    "\n",
    "| X | Y | AND(X,Y) \n",
    "|:---:|:---:|:---:|\n",
    "| 0 |0 | 0 |\n",
    "| 0 |1 | 0 |\n",
    "| 1 |0 | 0 |\n",
    "| 1 |1 | 1 |\n",
    "\n",
    "Assuming binary inputs, this truth table can also be described like this: \n",
    "\n",
    "<img src=\"andGraph.png\" width=\"200\" height=auto>\n",
    "\n",
    "We can draw the following decision boundary and now we have a region that matches our desired output.\n",
    "\n",
    "<img src=\"andGraph2.png\" width=\"200\" height=auto>\n",
    "\n",
    "This means we can express this logical operation with a neuron as well! \n",
    "\n",
    "<img src=\"andTotal.png\" width=\"600\" height=auto>\n",
    "\n",
    "One last trick to discuss. As we talked about in lecture, neural nets update their weights via backpropagation to improve the accuracy of the network's output. We want our networks to be able to adjust the activation function's parameters as well, so we are going to express them as normal weights. \n",
    "\n",
    "We do this by putting in an additional input, set to be -1 and weight it by our function's parameter. In this case, we are using the step function, so the weight will be the T value. Now our step function will use it's default parameter (T =0), but we have maintained the desired function by adding in this extra weighted input. \n",
    "\n",
    "Let's see how our AND neuron changes when we use this thresholding trick. \n",
    "\n",
    "<img src=\"andTotal2.png\" width=\"600\" height=auto>\n",
    "\n",
    "\n",
    "Now we can try and create some neural nets of our own.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def step(x):\n",
    "  return x > 0\n",
    "  \n",
    "def sigmoid(x, slope_at_zero=1):\n",
    "  return 1. / (1 + np.e ** (-4 * slope_at_zero * x))\n",
    "  \n",
    "def accuracy(desired_out, out):\n",
    "  return -.5 * (desired_out - out) ** 2\n",
    "\n",
    "class Neuron:\n",
    "  def __init__(self, name, input_weight_pairs):\n",
    "    self.name = str(name)\n",
    "    self.weight_dict = {input:weight for (input, weight) in input_weight_pairs}\n",
    "    self.inputs = [input for (input, weight) in self.weight_dict.items()]\n",
    "    self.neuron_inputs = [input for input in self.inputs if input == str(input)]\n",
    "  \n",
    "  # Recalculate neuron's output based on input neurons most recent outputs and threshold function\n",
    "  def call(self, output_dict, threshold_function):\n",
    "    output_dict[self.name] = threshold_function(sum([output_dict.get(input, input) * weight for (input, weight) in self.weight_dict.items()]))\n",
    "    \n",
    "  # Retrieve weight for a given input neuron\n",
    "  def get_weight(self, input):\n",
    "    return self.weight_dict[input]\n",
    "  \n",
    "  # Update weight for a given input neuron\n",
    "  def update_weight(self, input, new_weight):\n",
    "    self.weight_dict[input] = new_weight\n",
    "\n",
    "class NN:\n",
    "  def __init__(self, inputs, neuron_names, output_neuron, connections):\n",
    "    self.inputs = list(inputs)\n",
    "    self.connections = connections\n",
    "    self.neurons = {name:Neuron(name, [(from_neuron, weight) for (from_neuron, to_neuron, weight) in connections if to_neuron == name]) for name in neuron_names}\n",
    "    # self.output_neurons = [neuron for neuron in output_neurons if neuron in self.neurons else raise Exception(str(neuron) + ' is an output, but not in neuron list!')]\n",
    "    if output_neuron not in self.neurons:\n",
    "      raise Exception(str(neuron) + ' is selected as output neuron, but is not in neuron list!')\n",
    "    self.output_neuron = output_neuron\n",
    "    self.top_sorted_neurons = self.top_sort(neuron_names)\n",
    "  \n",
    "  def graph(self):\n",
    "    G = nx.DiGraph()\n",
    "    G.add_weighted_edges_from([(input, neuron_name, neuron.weight_dict[input]) for (neuron_name, neuron) in self.neurons.items() for input in neuron.inputs])\n",
    "    display_stn(G)\n",
    "\n",
    "    \n",
    "  # returns a topologically sorted list of neuron names in the neural net\n",
    "  def top_sort(self, neuron_names):\n",
    "    unsorted_neuron_names = set(neuron_names)\n",
    "    sorted_neuron_names = []\n",
    "    valid_inputs = set(self.inputs)\n",
    "    num_unsorted_neurons = len(unsorted_neuron_names)\n",
    "    while num_unsorted_neurons > 0:\n",
    "      for neuron in list(unsorted_neuron_names):\n",
    "        if all([input in valid_inputs for input in self.neurons[neuron].inputs]):\n",
    "          sorted_neuron_names.append(neuron)\n",
    "          valid_inputs.add(neuron)\n",
    "          unsorted_neuron_names.remove(neuron)\n",
    "      if len(unsorted_neuron_names) == num_unsorted_neurons:\n",
    "        raise Exception(str(num_unsorted_neurons) + ' neurons are unreachable or have unreachable inputs!')\n",
    "      num_unsorted_neurons = len(unsorted_neuron_names)\n",
    "    return sorted_neuron_names\n",
    "    \n",
    "  # returns a list of neurons with inputs connected to the output of the given neuron\n",
    "  def get_output_connections(self, neuron):\n",
    "    return [to_neuron for (from_neuron, to_neuron, weight) in self.connections if from_neuron == neuron]\n",
    "\n",
    "  # perform forward propogation on the neural net, returning the overall output and a dictionary of each neuron's output\n",
    "\n",
    "\n",
    "def plot_net(net, title, threshold_function=step):\n",
    "  # extract all non-numerical inputs (i.e. remove thresholds and constants)\n",
    "  # inputs = [input for input in raw_inputs if str(input) == input]\n",
    "  inputs = [input for input in net.inputs if str(input) == input]\n",
    "  # if there is only one string input (i.e. for NOT X implementation), create placeholder input for y axis\n",
    "  if len(inputs) == 1:\n",
    "    inputs.append('?')\n",
    "  x_min = 0\n",
    "  x_max = 1\n",
    "  y_min = 0\n",
    "  y_max = 1\n",
    "  divisions = 100\n",
    "  dict = {inputs[0]:1, inputs[1]:1}\n",
    "  map = np.zeros((divisions+1, divisions+1))\n",
    "  for x in range(divisions + 1):\n",
    "    for y in range(divisions + 1):\n",
    "      dict[inputs[0]] = x * float(x_max) / divisions + x_min\n",
    "      dict[inputs[1]] = y * float(y_max) / divisions + y_min\n",
    "      map[y,x] = myForwardProp(net,dict.copy(), threshold_function)[0]\n",
    "      \n",
    "  # plt.imshow(map, cmap='Greys',  interpolation='nearest')\n",
    "  # plt.imshow(map, cmap='Blues',  interpolation='nearest')\n",
    "  plt.imshow(map, cmap='Blues',  interpolation='bicubic')\n",
    "  plt.gca().invert_yaxis()\n",
    "  import matplotlib.ticker as tkr\n",
    "  format = tkr.FuncFormatter(lambda x,y: '{}'.format(x / float(divisions)))\n",
    "  plt.gca().xaxis.set_major_formatter(format)\n",
    "  plt.gca().yaxis.set_major_formatter(format)\n",
    "  plt.xlabel(inputs[0], fontsize=28)\n",
    "  plt.ylabel(inputs[1], fontsize=28)\n",
    "  plt.title(title, fontsize=38)\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the API\n",
    "\n",
    "Our API is currently incomplete! Don't worry, you'll soon be adding in the missing functions. \n",
    "\n",
    "But first, let's walk through how we would use our API to construct the AND neuron discussed above. \n",
    "\n",
    "This is to give you an understanding of how the API works, but again, remember that this code will not run until you have completed the missing functionality. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"143pt\" viewBox=\"0.00 0.00 125.00 143.33\" width=\"125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(0.833333 0.833333) rotate(0) translate(4 168)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-168 146,-168 146,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- y -->\n",
       "<g class=\"node\" id=\"node1\"><title>y</title>\n",
       "<ellipse cx=\"22\" cy=\"-143\" fill=\"none\" rx=\"21.5\" ry=\"21.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"22\" y=\"-139.3\">y</text>\n",
       "</g>\n",
       "<!-- n1 -->\n",
       "<g class=\"node\" id=\"node2\"><title>n1</title>\n",
       "<ellipse cx=\"120\" cy=\"-82\" fill=\"none\" rx=\"21.5\" ry=\"21.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"120\" y=\"-78.3\">n1</text>\n",
       "</g>\n",
       "<!-- y&#45;&gt;n1 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>y-&gt;n1</title>\n",
       "<path d=\"M40.6079,-131.812C55.2229,-122.525 76.2238,-109.181 92.8112,-98.6408\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"94.9406,-101.435 101.504,-93.1174 91.1864,-95.5264 94.9406,-101.435\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"71\" y=\"-120.8\">1.0</text>\n",
       "</g>\n",
       "<!-- &#45;1 -->\n",
       "<g class=\"node\" id=\"node3\"><title>-1</title>\n",
       "<ellipse cx=\"22\" cy=\"-82\" fill=\"none\" rx=\"21.5\" ry=\"21.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"22\" y=\"-78.3\">-1</text>\n",
       "</g>\n",
       "<!-- &#45;1&#45;&gt;n1 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>-1-&gt;n1</title>\n",
       "<path d=\"M43.8306,-82C56.788,-82 73.684,-82 88.1745,-82\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"88.2101,-85.5001 98.2101,-82 88.21,-78.5001 88.2101,-85.5001\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"71\" y=\"-85.8\">1.5</text>\n",
       "</g>\n",
       "<!-- x -->\n",
       "<g class=\"node\" id=\"node4\"><title>x</title>\n",
       "<ellipse cx=\"22\" cy=\"-21\" fill=\"none\" rx=\"21.5\" ry=\"21.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"22\" y=\"-17.3\">x</text>\n",
       "</g>\n",
       "<!-- x&#45;&gt;n1 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>x-&gt;n1</title>\n",
       "<path d=\"M40.6079,-32.1883C55.2229,-41.475 76.2238,-54.8193 92.8112,-65.3592\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"91.1864,-68.4736 101.504,-70.8826 94.9406,-62.5654 91.1864,-68.4736\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"71\" y=\"-59.8\">1.0</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "global name 'myForwardProp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5cfccbdf8f4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m#To plot the decision boundaries drawn, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mplot_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mand_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'X AND Y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m#The last argument is the activation function. We are using the step function,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-bb06b6f79779>\u001b[0m in \u001b[0;36mplot_net\u001b[1;34m(net, title, threshold_function)\u001b[0m\n\u001b[0;32m     86\u001b[0m       \u001b[0mdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_max\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mdivisions\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx_min\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m       \u001b[0mdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_max\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mdivisions\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my_min\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m       \u001b[0mmap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyForwardProp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m   \u001b[1;31m# plt.imshow(map, cmap='Greys',  interpolation='nearest')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'myForwardProp' is not defined"
     ]
    }
   ],
   "source": [
    "#Firstly, we want to specify the  3 inputs: x, y, and the -1 for the thresholding trick. \n",
    "inputs = ['x', 'y', -1]\n",
    "\n",
    "#We then need to specify the neurons we want in the network by providing a list of string identifiers.\n",
    "#In this case, we only need one neuron. \n",
    "neurons = ['n1']\n",
    "\n",
    "#We also want to specify which neuron is the output neuron. Again, since we are only using one neuron, \n",
    "#n1 is also our output neuron. \n",
    "output_neuron = 'n1'\n",
    "\n",
    "#The next step is to wire the net. We make connections in the following format:\n",
    "#(input, neuron, weight_on_input )\n",
    "#Note that if you wanted to connect two neurons together, you could have:\n",
    "#('nx', 'ny', 1.0)\n",
    "\n",
    "#We want all 3 inputs to connect to our single neuron. x and y are weighted by 1 and -1 is weighted by 1.5, the threshold\n",
    "connections = [('x', 'n1',  1),\n",
    "               ('y', 'n1',  1),\n",
    "               (-1, 'n1', 1.5)]\n",
    "\n",
    "#Finally we can initialize our net like so:\n",
    "and_net = NN(inputs, neurons, output_neuron, connections)\n",
    "\n",
    "#To visualize your neural network, run:\n",
    "and_net.graph()\n",
    "\n",
    "#To plot the decision boundaries drawn, call:\n",
    "plot_net(and_net, 'X AND Y', step)\n",
    "\n",
    "#The last argument is the activation function. We are using the step function, \n",
    "#but you can also use the provided sigmoid function - plot_net(and_net, 'X AND Y', sigmoid)\n",
    "# 'X AND Y' is the title string\n",
    "\n",
    "#NOTE: This will not run until you have completed forward propagation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "Now that you're familiar with how the API creates a neural net, its time to complete one of the missing functions!\n",
    "\n",
    "You will now implement the process of Forward Propagation. This process is essentially \"running\" the network. For each neuron, we must take the inputs, perform the weighted sum, and run the activation function. We start with the neurons in the input layer and work through the network until we can calculate the final output. \n",
    "\n",
    "Use the existing functions in the API (the topological sort might be of particular usefulness) to program forward propagation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# perform forward propogation on the neural net, returning the overall output and a dictionary of each neuron's output\n",
    "def myForwardProp(neuralNet, input_value_dict, threshold_function):\n",
    "    # neuralNet is the Neural Net Class Instance\n",
    "    # input_value_dict is a copy of the input dictionary, mapping input variables to values\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the following code to test your implementation of forward propagation. All the logical operator plots will also use this code, so you can use those results as additional test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-021223b5858e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m                ( -1, 'n10',  .5)]\n\u001b[0;32m     34\u001b[0m \u001b[0mxor_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneurons\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_neuron\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconnections\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mplot_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxor_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'X XOR Y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;31m# xor_net.graph()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-12c6e79b7619>\u001b[0m in \u001b[0;36mplot_net\u001b[1;34m(net, title, threshold_function)\u001b[0m\n\u001b[0;32m     91\u001b[0m       \u001b[0mdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_max\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mdivisions\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx_min\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m       \u001b[0mdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_max\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mdivisions\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my_min\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m       \u001b[0mmap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyForwardProp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m   \u001b[1;31m# plt.imshow(map, cmap='Greys',  interpolation='nearest')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-2931ebce2e8a>\u001b[0m in \u001b[0;36mmyForwardProp\u001b[1;34m(neuralNet, in_dict, threshold_function)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# neuralNet is the Neural Net Class Instance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# in_dict is a copy of the input dictionary, mapping input variables to values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# triangle and rectangle\n",
    "inputs = ['x', 'y', -1]\n",
    "neurons = ['n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10']\n",
    "output_neuron = 'n10'\n",
    "connections = [('x', 'n1', -1),\n",
    "               ('y', 'n1', -1),\n",
    "               ( -1, 'n1', -1.5),\n",
    "               ('x', 'n2', 1),\n",
    "               ('y', 'n2', -1),\n",
    "               ( -1, 'n2', -.5),\n",
    "               ('x', 'n3', 0),\n",
    "               ('y', 'n3', 1),\n",
    "               ( -1, 'n3', .5),\n",
    "               ('n1', 'n4', 1),\n",
    "               ('n2', 'n4', 1),\n",
    "               ('n3', 'n4', 1),\n",
    "               ( -1, 'n4',  2.5),\n",
    "               ('x', 'n5', -1),\n",
    "               ( -1, 'n5', -.75),\n",
    "               ('x', 'n6', 1),\n",
    "               ( -1, 'n6', .25),\n",
    "               ('y', 'n7', -1),\n",
    "               ( -1, 'n7', -.375),\n",
    "               ('y', 'n8', 1),\n",
    "               ( -1, 'n8', .125),\n",
    "               ('n5', 'n9', 1),\n",
    "               ('n6', 'n9', 1),\n",
    "               ('n7', 'n9', 1),\n",
    "               ('n8', 'n9', 1),\n",
    "               ( -1, 'n9',  3.5),\n",
    "               ('n4', 'n10', 1),\n",
    "               ('n9', 'n10', 1),\n",
    "               ( -1, 'n10',  .5)]\n",
    "xor_net = NN(inputs, neurons, output_neuron, connections)\n",
    "plot_net(xor_net, 'X XOR Y', step)\n",
    "# xor_net.graph()\n",
    "\n",
    "# verify forward prop\n",
    "dict = {'x':1, 'y':1}\n",
    "checks = [((.5,.75), 1), \n",
    "          ((.5,.45), 0), \n",
    "          ((.5,.25), 1), \n",
    "          ((.5,.08), 0), \n",
    "          ((.1,.75), 0), \n",
    "          ((.9,.75), 0), \n",
    "          ((.1,.25), 0), \n",
    "          ((.9,.25), 0)]\n",
    "for ((x,y), out) in checks:\n",
    "    dict['x'] = x\n",
    "    dict['y'] = y\n",
    "    net_output = myForwardProp(xor_net,dict, step)[0]\n",
    "    if net_output != out:\n",
    "      raise Exception('Failed on (' + str(x) + ', ' + str(y) + '): ' + str(out) + ', output was: ' + str(net_output))\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logical Operators\n",
    "\n",
    "Now that you've completed the forward propagation, the AND neuron we built earlier can be run. \n",
    "\n",
    "You will now create the neurons for OR and NOT(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#define necessary aspects here\n",
    "\n",
    "\n",
    "#confirm your answer by checking the resulting plot\n",
    "\n",
    "#or_net = NN(inputs, neurons, output_neuron, connections)\n",
    "#or_net.graph()\n",
    "#plot_net(or_net, 'X OR Y', step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define necessary aspects here\n",
    "\n",
    "\n",
    "#confirm your answer by checking the resulting plot\n",
    "\n",
    "#notX_net = NN(inputs, neurons, output_neuron, connections)\n",
    "#notX_net.graph()\n",
    "#plot_net(notX_net, 'NOT X', step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR\n",
    "\n",
    "Now you will implement XOR, but this operator is a little more complicated than the previous ones.\n",
    "\n",
    "You will need to draw two decision boundaries and combine them using one of the other logical operations. \n",
    "\n",
    "As we talked about before, this means that implementing XOR will require more than one neuron. \n",
    "\n",
    "<img src=\"xor.png\" width=\"200\" height=auto>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define necessary aspects here\n",
    "\n",
    "\n",
    "#confirm your answer by checking the resulting plot\n",
    "\n",
    "#xor_net = NN(inputs, neurons, output_neuron, connections)\n",
    "#xor_net.graph()\n",
    "#plot_net(xor_net, 'X XOR Y', step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Backpropagation\n",
    "\n",
    "Lastly, you will implement backpropagation using the sigmoid function as the activation function. \n",
    "\n",
    "\n",
    "The sigmoid function has some nice mathematical properties, so calculating the weights using the procedure described in lecture results in a simple formula, which we have computed for you. \n",
    "\n",
    "<img src=\"equations.png\" width=\"700\" height=auto>\n",
    "\n",
    "To run backpropagation, you will:\n",
    "\n",
    " 1. Run forward propagation and save the outputs of every neuron\n",
    " 2. Compute the delta value for each neuron, starting from the output neuron and working backwards, towards the input neurons\n",
    " 3. Calculate the new weights using the delta values and update them in the network.\n",
    " \n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For the following functions:\n",
    "\n",
    "# neuralNet is a Neural Net Class Instance\n",
    "# input_value_dict is an input dictionary, mapping input variables to values\n",
    "# output_dict needs to be a mapping every neuron in the network to its output\n",
    "\n",
    "# Helper Function for backward_prop- returns a dictionary of deltas, one for each neuron in the network\n",
    "def compute_deltas(neuralNet, input_value_dict, output_dict, desired_out):\n",
    "    raise NotImplementedError\n",
    "\n",
    "# Helper Function for backward_prop- updates all of the weights in the network, doesn't return anything\n",
    "def update_weights(neuralNet, input_value_dict, output_dict, desired_out, r):\n",
    "    raise NotImplementedError\n",
    "    \n",
    "# Perform backward propogation on the neural net, and changes the weights in the network - it doesn't return anything\n",
    "def myBackwardProp(neuralNet, input_value_dict, desired_out, threshold_function=sigmoid, r=1, max_error=-.0001):\n",
    "    raise NotImplementedError\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following code to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-20df8b6d5fec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m                ( -1, 'n10',  .5)]\n\u001b[0;32m     34\u001b[0m \u001b[0mxor_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneurons\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_neuron\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconnections\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mplot_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxor_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'X XOR Y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;31m# xor_net.graph()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-12c6e79b7619>\u001b[0m in \u001b[0;36mplot_net\u001b[1;34m(net, title, threshold_function)\u001b[0m\n\u001b[0;32m     91\u001b[0m       \u001b[0mdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_max\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mdivisions\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx_min\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m       \u001b[0mdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_max\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mdivisions\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my_min\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m       \u001b[0mmap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyForwardProp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m   \u001b[1;31m# plt.imshow(map, cmap='Greys',  interpolation='nearest')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-2931ebce2e8a>\u001b[0m in \u001b[0;36mmyForwardProp\u001b[1;34m(neuralNet, in_dict, threshold_function)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# neuralNet is the Neural Net Class Instance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# in_dict is a copy of the input dictionary, mapping input variables to values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# triangle and rectangle\n",
    "inputs = ['x', 'y', -1]\n",
    "neurons = ['n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10']\n",
    "output_neuron = 'n10'\n",
    "connections = [('x', 'n1', -1),\n",
    "               ('y', 'n1', -1),\n",
    "               ( -1, 'n1', -1.5),\n",
    "               ('x', 'n2', 1),\n",
    "               ('y', 'n2', -1),\n",
    "               ( -1, 'n2', -.5),\n",
    "               ('x', 'n3', 0),\n",
    "               ('y', 'n3', 1),\n",
    "               ( -1, 'n3', .5),\n",
    "               ('n1', 'n4', 1),\n",
    "               ('n2', 'n4', 1),\n",
    "               ('n3', 'n4', 1),\n",
    "               ( -1, 'n4',  2.5),\n",
    "               ('x', 'n5', -1),\n",
    "               ( -1, 'n5', -.75),\n",
    "               ('x', 'n6', 1),\n",
    "               ( -1, 'n6', .25),\n",
    "               ('y', 'n7', -1),\n",
    "               ( -1, 'n7', -.375),\n",
    "               ('y', 'n8', 1),\n",
    "               ( -1, 'n8', .125),\n",
    "               ('n5', 'n9', 1),\n",
    "               ('n6', 'n9', 1),\n",
    "               ('n7', 'n9', 1),\n",
    "               ('n8', 'n9', 1),\n",
    "               ( -1, 'n9',  3.5),\n",
    "               ('n4', 'n10', 1),\n",
    "               ('n9', 'n10', 1),\n",
    "               ( -1, 'n10',  .5)]\n",
    "xor_net = NN(inputs, neurons, output_neuron, connections)\n",
    "plot_net(xor_net, 'X XOR Y', step)\n",
    "# xor_net.graph()\n",
    "\n",
    "# verify backward prop\n",
    "input_value_dict = {'x':.5, 'y':.45}\n",
    "desired_out = 1\n",
    "checks = [('n10', 'n9', 1.0580876267998136),\n",
    "          ('n10', 'n4', 1.750519436878936),\n",
    "          ('n10', -1, 0.5),\n",
    "          ('n8', 'y', 1.004405571242847),\n",
    "          ('n8', -1, 0.125),\n",
    "          ('n9', 'n8', 1.0448828563571808),\n",
    "          ('n9', -1, 3.5),\n",
    "          ('n9', 'n5', 1.0417815987312213),\n",
    "          ('n9', 'n6', 1.0417815987312213),\n",
    "          ('n9', 'n7', 1.024362381541987),\n",
    "          ('n1', 'y', -0.9842872685460131),\n",
    "          ('n1', 'x', -0.9825414094955703),\n",
    "          ('n1', -1, -1.5),\n",
    "          ('n2', 'y', -0.9842872685460131),\n",
    "          ('n2', 'x', 1.0174585905044304),\n",
    "          ('n2', -1, -0.5),\n",
    "          ('n3', 'y', 1.0418161693022556),\n",
    "          ('n3', 'x', 0.04646241033584111),\n",
    "          ('n3', -1, 0.5),\n",
    "          ('n4', 'n1', 1.3130856737676413),\n",
    "          ('n4', 'n2', 1.3130856737676413),\n",
    "          ('n4', 'n3', 1.162829074164861),\n",
    "          ('n4', -1, 2.5),\n",
    "          ('n5', 'x', -0.9942918288280811),\n",
    "          ('n5', -1, -0.75),\n",
    "          ('n6', 'x', 1.005708171171915),\n",
    "          ('n6', -1, 0.25),\n",
    "          ('n7', 'y', -0.993643573887871),\n",
    "          ('n7', -1, -0.375)]\n",
    "\n",
    "myBackwardProp(xor_net,input_value_dict, desired_out)\n",
    "for (neuron_name, input, weight) in checks:\n",
    "  weight_out = xor_net.neurons[neuron_name].weight_dict[input]\n",
    "  if abs(weight_out - weight) > .001:\n",
    "    raise Exception('Failed on ' + str(neuron_name) + ' with input ' + str(input) + ' and weight ' + str(weight) + ', weight was: ' + str(weight_out))\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Congrats! \n",
    "\n",
    "You are done with Part 1 of the Pset!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
